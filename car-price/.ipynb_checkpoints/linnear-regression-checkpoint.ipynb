{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e829091-010c-48b4-8c75-176a43c91a74",
   "metadata": {},
   "source": [
    "$g(x_1) = g(x_{i1}, x_{i2}, ..., x_{in}) = w_0 + \\sum_{j=1}^n x_{ij}w_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d6b08-77a1-4cca-b64d-2b243672fe14",
   "metadata": {},
   "source": [
    "where  \n",
    "$w_0$ is the bias term  \n",
    "$w_1,w_2....,w_n$ are the weights for each feature $x_{i1}, x_{i2}, ..., x_{in}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e8d21-7620-44dc-ba6b-14771511d030",
   "metadata": {},
   "source": [
    "As example for machine learning let's use this values  \n",
    "$w_0 = 7.17$  \n",
    "$w_1 = 0.01$  \n",
    "$w_2 = 0.04$  \n",
    "$w_3 = 0.002$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2af0246-fd8b-44d4-9564-75aa63996475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = 7.17\n",
    "## [ w1,  w2,    w3]\n",
    "w = [0.01, 0.04, 0.002]\n",
    "n = 3 \n",
    "\n",
    "xi = [453, 11, 86] ## values as example\n",
    "\n",
    "def linear_regression(xi):\n",
    "    result = w0\n",
    "    for j in range(n):\n",
    "        result = result + xi[j] * w[j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0f0de-93e4-4401-a303-22ae6f8885a7",
   "metadata": {},
   "source": [
    "The bias term is the value we would predict if we didn't know anything about; it serves as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377e54d-5d8c-4e4c-ab63-c5a98599bb39",
   "metadata": {},
   "source": [
    "Because we now think of both features and weights as vectors $x_i$ and $w$, respectively, we can replace the sum of the elements of these vectors with a dot product between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd7ead1-5aae-4b46-9b4e-395aa53b11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(xi, w):\n",
    "    n = len(w)\n",
    "    result = 0.0\n",
    "    for j in range(n):\n",
    "        result = result + xi[j] * w[j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19f6fc-96d3-4833-a383-ad44433188df",
   "metadata": {},
   "source": [
    "Using the new notation, we can rewrite the entire equation for linear regresion as:  \n",
    "$g(x_i) = w_0 + x^T_i w$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b650d-523c-459c-984e-3f6cadd52df8",
   "metadata": {},
   "source": [
    "Now we can use the new **dot** function, so the linear regresion function in Python becomes very short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2741ebb-ce89-404e-b2de-fca819db6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xi):\n",
    "    return w0 + dot(xi,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb49197-14c7-4682-9504-375bcd47240c",
   "metadata": {},
   "source": [
    "Alternatively, if __xi__ and __w__ are NumPy arrays, we can use the built in __dot__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d129f41-52b5-4d78-a696-70295f533c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regresion(xi):\n",
    "    return w0 + xi.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b502c9-e3a7-459d-9c1a-e8c4b0ef01e4",
   "metadata": {},
   "source": [
    "To make it even shorter, we can combine $w_0$ and $w$ into one (n+1)-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc63e9b-a0bd-447d-a6ae-a156f180a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w0] + w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f2877-4305-4789-b6fd-89fef1ddaab4",
   "metadata": {},
   "source": [
    "Because now w becomes a (n+1)-dimensional vector, we also need to adjust the feature vector $x_i$ so that the dot  product between them still works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed68df14-4294-4f21-89f3-bd80c4ff009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = [1] + xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7cf59-47e6-4c1c-9f43-225700a5cc74",
   "metadata": {},
   "source": [
    "With these modifications, we can express the model as the dot product between the new $x_i$ and the new $w$:  \n",
    "$g(x_i) = x^T_iw$  \n",
    "The translation to the code is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2baac38-d7ed-41bc-884f-27a17d5b2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 7.17\n",
    "w = [0.01,0.04,0.002]\n",
    "w = [w0] + w\n",
    "\n",
    "def linear_regression(xi):\n",
    "    xi = [1] + xi\n",
    "    return dot(xi,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeaf0d4-f8d2-4a4b-bc62-a3a8f9804dff",
   "metadata": {},
   "source": [
    "Let's talk about the matrix form. There are many observations and $x_i$ is one of them. Thus, we have $m$ feature vectors $x_1, x_2, ... , x_i, ..., x_m$ and each of these vectors consists of $n+1$ features. We can put these vectors together as rows of a matrix. Let's call this matrix X.\n",
    "Let's see how it looks in code, we can take a few rows from the training dataset, such as the first, second and tenth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36649c14-8dd4-4880-a1f9-cf48ebd9a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1  = [1, 148, 24, 1385]\n",
    "x2  = [1, 132, 25, 2031]\n",
    "x10 = [1, 453, 11, 86]\n",
    "\n",
    "## now let's put the rows together in another list\n",
    "X = [x1, x2, x10]\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bf77a-2b7b-4f0a-94dc-3becf0fcbfd0",
   "metadata": {},
   "source": [
    "We alrade learn that to make a prediction for a single feature vector, we need to calculate the dot product between this feature vector and the weigths vector.  \n",
    "To make predictions for all the rows of the matrix, we can simply iterate over all rows of X and compute the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6a6964-6c59-400c-959f-89116e6e31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for xi in X:\n",
    "    pred = dot(xi, w)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1d95b-1b49-4aef-ac7d-52fe21ef15bd",
   "metadata": {},
   "source": [
    "In linear algebra, this is the matrix-vector multiplications: we multiply the matrix X by the vector w. THe formula for linear regression becomes:  \n",
    "$g(X) = w_0 + Xw$  \n",
    "The result is an array with predictions for each roe of X.  \n",
    "With this matrix formulation, the code for applying linear regression to make predictions becomes very simple. THe translation to NumPy becomes straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48550d07-8724-4813-a29e-81d135fe389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.38 , 13.552, 12.312])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a2e40-5081-4065-b365-38251001eb42",
   "metadata": {},
   "source": [
    "HOw do we get the weights w ?  \n",
    "We will use normal equation, which is the simplest method to implement:  \n",
    "$w = (X^T X)^{-1}X^T y$  \n",
    "$X^T$ is the transpose of X. in NumPy is: X.T  \n",
    "$X^TX$ in NumPy: X.T.dot(X)  \n",
    "$X^{-1}$ is the inverse of X, we can use np.linalg.inv  \n",
    "So the formula translates directly to: inv(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d734524f-6515-4a92-a80c-34ce550d40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regresion WIth NumPy\n",
    "def train_linear_regression(X,y):\n",
    "    # Adding the dummy column\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones,X]) #Adds the array of 1's as the first column of X\n",
    "    \n",
    "    # Normal Equation Formula\n",
    "    XTX = X.T.dot(X) # Computes X^TX\n",
    "    XTX_inv = np.linalg.inv(XTX) # Computes the inverse of X^TX\n",
    "    w = XTX_inv.dot(X.T).dot(y) # Computes the rest of the normal equation\n",
    "    \n",
    "    return w[0], w[1:] # Splits the weights vector into the bias and the rest of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c74a6f-5e19-41b0-9d4f-7f1b79b5985d",
   "metadata": {},
   "source": [
    "If weights are split into the bias term and the rest, the linear regression formula for making predictions changes slightly  \n",
    "$g(X) = w_0 + Xw$\n",
    "this is still very easy to translate to NumPy:\n",
    "y_pred = w0 + X.dot(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
